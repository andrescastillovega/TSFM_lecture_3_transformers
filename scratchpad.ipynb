{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1573d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tokenizer.EncoderDecoder import encoder, decoder\n",
    "from tokenizer.utils import load_bpe, render_token\n",
    "\n",
    "from io import BufferedReader\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, trange\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c179cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPROC = 16\n",
    "VOCAB_SIZE = 1256\n",
    "D = 512 # model dimension (embeddings)\n",
    "S = 100 # Sequence lenght (context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f454a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7729d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_boundaries(file_path: str, special_token: str = '<|endoftext|>'):\n",
    "    f: BufferedReader = open(file_path, 'rb')\n",
    "    content = f.read()\n",
    "    f.close()    \n",
    "\n",
    "    chunk_starts = [0]\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        match_index = content.find(special_token.encode('utf-8'), start_index)\n",
    "        if match_index == -1:\n",
    "            break\n",
    "        chunk_starts.append(match_index + len(special_token))\n",
    "        start_index = match_index + len(special_token)\n",
    "    chunk_ends = chunk_starts[1:]\n",
    "    chunk_ends.append(len(content))\n",
    "    chunk_boundaries = list(zip(chunk_starts, chunk_ends))\n",
    "\n",
    "    return chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbd8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chunk(chunk_boundaries: tuple, file_path: str, model: str):\n",
    "    f = open(file_path, 'rb')\n",
    "    chunk_start, chunk_end = chunk_boundaries\n",
    "    \n",
    "    f.seek(chunk_start)\n",
    "\n",
    "    tokens = encoder(f.read(chunk_end-chunk_start).decode(), model=model)\n",
    "    f.close()\n",
    "    return tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e2a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_train_data(file_path: str, tokenizer:str):\n",
    "    encode_f = functools.partial(encode_chunk, file_path=file_path, model=tokenizer)\n",
    "\n",
    "    chunks_boundaries = get_chunk_boundaries(file_path=file_path)\n",
    "\n",
    "    chunks_tokens = []\n",
    "    with multiprocessing.Pool(NPROC) as p:\n",
    "        for chunk in tqdm(p.imap_unordered(encode_f, chunks_boundaries), total=len(chunks_boundaries), desc=\"Tokenizing\"):\n",
    "            chunks_tokens.append(chunk)\n",
    "        p.close()\n",
    "        p.join()\n",
    "    \n",
    "    return chunks_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b915c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 460/460 [00:04<00:00, 95.67it/s] \n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_train_data('tokenizer/data/toy_data.txt', tokenizer='tinystories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c8afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenghts = [ lenght for _, lenght in tokens ]\n",
    "plt.hist(lenghts, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c9c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(chunks: list, B: int, S: int, stride: int):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for tokens, lenght in chunks:\n",
    "        for idx in range(0, lenght - S - 1, stride):\n",
    "            inputs.append(tokens[idx:idx + S])\n",
    "            targets.append(tokens[idx+1:idx + S + 1])\n",
    "\n",
    "    for i in range(0, len(inputs), B):\n",
    "        batch_inputs = inputs[i:i + B]\n",
    "        batch_targets = targets[i:i + B]\n",
    "        \n",
    "        if len(batch_inputs) == B:\n",
    "            yield batch_inputs, batch_targets\n",
    "\n",
    "    return batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621d868",
   "metadata": {},
   "source": [
    "# Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf85842",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2becb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 3           # Sequence lenght\n",
    "D = 8           # Model dimension (embedding size)\n",
    "N_HEADS = 2\n",
    "D_K = D // N_HEADS         # Dimension of keys (used to scale attention)\n",
    "B = 2\n",
    "NBATCHES = 1\n",
    "VOCAB_SIZE = 10\n",
    "\n",
    "W_TE = np.random.rand(VOCAB_SIZE, D) # VOCAB_SIZE x D\n",
    "W_PE = np.random.rand(S, D)\n",
    "\n",
    "W_Q = np.random.rand(D, D_K * N_HEADS)\n",
    "W_K = np.random.rand(D, D_K * N_HEADS)\n",
    "W_V = np.random.rand(D, D_K * N_HEADS)\n",
    "W_O = np.random.rand(D, D)\n",
    "\n",
    "GAMMA = np.random.rand(B, S, 1)\n",
    "BETA = np.random.rand(B, S, 1)\n",
    "\n",
    "FFN_EXPCON_FAC = 4\n",
    "W_FFN_E = np.random.rand(D, FFN_EXPCON_FAC * D)\n",
    "W_FFN_C = np.random.rand(FFN_EXPCON_FAC * D, D)\n",
    "\n",
    "W_LOGITS = np.random.rand(B, D, VOCAB_SIZE)\n",
    "\n",
    "# GAMMA = np.ones((B,S,1))\n",
    "# BETA = np.zeros((B,S,1))\n",
    "\n",
    "# W_O = \n",
    "# W_FF_expand = \n",
    "# W_FF_contract =\n",
    "# gamma =\n",
    "# beta ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94eedb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import (\n",
    "    softmax,\n",
    "    relu,\n",
    "    qkv_projection,\n",
    "    layer_norm,\n",
    "    feed_forward_network,\n",
    "    block_forward,\n",
    "    multi_head_attention_backward,\n",
    "    softmax_backward_from_probs   \n",
    ")\n",
    "\n",
    "from transformer import multi_head_attention as mha_np\n",
    "from transformer import multi_head_attention_backward as mha_back_np\n",
    "\n",
    "from torch_primitives import torch_multi_head_attention_backward as mha_back_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91304d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    assert q.shape == k.shape\n",
    "\n",
    "    # NOTE: swapaxes(1,2) is a final step after reshape to split the q matrix into each head\n",
    "    #       the final swapaxes(2,3) in tensor k is for transpose in the attention score calculation\n",
    "    attn_scores = q.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) @ k.reshape(B,S,N_HEADS,D_K).swapaxes(1,2).swapaxes(2,3)\n",
    "    mask = np.tril(np.ones(attn_scores.shape), k=0).astype(bool)\n",
    "    attn_scores_masked = np.where(mask, attn_scores, -np.inf)\n",
    "\n",
    "    scale = 1.0 / np.sqrt(q.shape[-1])\n",
    "    causal_attn = softmax(attn_scores_masked * scale)\n",
    "\n",
    "    context_vectors = causal_attn @ v.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) # Applies same reshape and swap as q & k\n",
    "\n",
    "    return context_vectors.swapaxes(1,2).reshape(B,S,D_K * N_HEADS) # Concat context vector from each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8856c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_forward(x: np.ndarray):\n",
    "    q, k, v = qkv_projection(x, W_Q, W_K, W_V)\n",
    "    print(f\"qkv_proj: {(x.shape, q.shape, k.shape, v.shape)}\")\n",
    "\n",
    "    context_vec = multi_head_attention(q, k, v)\n",
    "    attn_out = context_vec @ W_O\n",
    "    print(f\"causal attn: {attn_out.shape}\")\n",
    "\n",
    "    ln_attn = layer_norm(x + attn_out, GAMMA, BETA)\n",
    "    print(f\"layer norm (attn): {ln_attn.shape}\")\n",
    "\n",
    "    ffn = feed_forward_network(ln_attn, W_FFN_E, W_FFN_C)\n",
    "    print(f\"ffn: {ffn.shape}\")\n",
    "\n",
    "    ln_head = layer_norm(ln_attn + ffn, GAMMA, BETA)\n",
    "    print(f\"layer norm head: {ln_head.shape}\")\n",
    "\n",
    "    return ln_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055e12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (1, 2, 3)\n",
      "input emb: (1, 2, 3, 8)\n",
      "qkv_proj: ((2, 3, 8), (2, 3, 8), (2, 3, 8), (2, 3, 8))\n",
      "causal attn: (2, 3, 8)\n",
      "layer norm (attn): (2, 3, 8)\n",
      "ffn: (2, 3, 8)\n",
      "layer norm head: (2, 3, 8)\n",
      "qkv_proj: ((2, 3, 8), (2, 3, 8), (2, 3, 8), (2, 3, 8))\n",
      "causal attn: (2, 3, 8)\n",
      "layer norm (attn): (2, 3, 8)\n",
      "ffn: (2, 3, 8)\n",
      "layer norm head: (2, 3, 8)\n",
      "qkv_proj: ((2, 3, 8), (2, 3, 8), (2, 3, 8), (2, 3, 8))\n",
      "causal attn: (2, 3, 8)\n",
      "layer norm (attn): (2, 3, 8)\n",
      "ffn: (2, 3, 8)\n",
      "layer norm head: (2, 3, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randint(1, VOCAB_SIZE, (NBATCHES,B,S))\n",
    "\n",
    "input_emb = W_TE[data] + W_PE[np.arange(S)] # S x D\n",
    "print(f\"data: {data.shape}\")\n",
    "print(f\"input emb: {input_emb.shape}\")\n",
    "\n",
    "for batch in input_emb:\n",
    "    l1 = block_forward(batch)\n",
    "    l2 = block_forward(l1)\n",
    "    l3 = block_forward(l2)\n",
    "    logits = softmax(l3 @ W_LOGITS)\n",
    "    tokens = np.argmax(logits[:-1], axis=-1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be81d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "q = np.random.rand(2,1,3,4)\n",
    "k = np.random.rand(2,1,3,4)\n",
    "v = np.random.rand(2,1,3,4)\n",
    "\n",
    "dOut = np.random.rand(2,1,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173045f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.51948512, 0.61289453, 0.12062867, 0.8263408 ],\n",
       "         [0.55812428, 0.58153635, 0.22332821, 0.5849033 ],\n",
       "         [0.51260863, 0.61351953, 0.4474817 , 0.54803669]]],\n",
       "\n",
       "\n",
       "       [[[0.66931378, 0.58593655, 0.6249035 , 0.67468905],\n",
       "         [0.74549826, 0.36457956, 0.68600801, 0.48490987],\n",
       "         [0.54490799, 0.45948477, 0.47326716, 0.64405419]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_np(q, k, v, unmasked=False)#.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "474086ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 24 into shape (1,4,2,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmha_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.sum(axis=-1, keepdims=True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/TSFM/lecture-three/lecture-three/transformer.py:116\u001b[39m, in \u001b[36mmulti_head_attention\u001b[39m\u001b[34m(q, k, v)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m q.shape == k.shape\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# NOTE: swapaxes(1,2) is a final step after reshape to split the q matrix into each head\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#       the final swapaxes(2,3) in tensor k is for transpose in the attention score calculation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m attn_scores = \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN_HEADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD_K\u001b[49m\u001b[43m)\u001b[49m.swapaxes(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m) @ k.reshape(B,S,N_HEADS,D_K).swapaxes(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m).swapaxes(\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m)\n\u001b[32m    117\u001b[39m mask = np.tril(np.ones((S,S)), k=\u001b[32m0\u001b[39m).astype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m    118\u001b[39m attn_scores_masked = np.where(mask, attn_scores, -np.inf)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 24 into shape (1,4,2,8)"
     ]
    }
   ],
   "source": [
    "mha_np(q, k, v)#.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8708d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23f010d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_torch(q, k, v):\n",
    "    q_torch = torch.from_numpy(q).float().requires_grad_(True)\n",
    "    k_torch = torch.from_numpy(k).float().requires_grad_(True)\n",
    "    v_torch = torch.from_numpy(v).float().requires_grad_(True)\n",
    "\n",
    "    # Multi-head attention forward pass\n",
    "    d_h = q_torch.shape[-1]\n",
    "    mask = torch.triu(torch.ones(2,3,3), diagonal=1)\n",
    "    attn_scores = q_torch @ k_torch.transpose(-2, -1) / np.sqrt(d_h)\n",
    "    causal_attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "    attn_weights = F.softmax(causal_attn_scores, dim=-1)\n",
    "    output_torch = attn_weights @ v_torch\n",
    "\n",
    "    return output_torch\n",
    "\n",
    "# output_torch.backward(dOut_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff8038f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5195, 0.6129, 0.1206, 0.8263],\n",
       "          [0.5581, 0.5815, 0.2233, 0.5849],\n",
       "          [0.5126, 0.6135, 0.4475, 0.5480]],\n",
       "\n",
       "         [[0.5195, 0.6129, 0.1206, 0.8263],\n",
       "          [0.5581, 0.5815, 0.2233, 0.5849],\n",
       "          [0.5126, 0.6135, 0.4475, 0.5480]]],\n",
       "\n",
       "\n",
       "        [[[0.6693, 0.5859, 0.6249, 0.6747],\n",
       "          [0.7455, 0.3646, 0.6860, 0.4849],\n",
       "          [0.5449, 0.4595, 0.4733, 0.6441]],\n",
       "\n",
       "         [[0.6693, 0.5859, 0.6249, 0.6747],\n",
       "          [0.7455, 0.3646, 0.6860, 0.4849],\n",
       "          [0.5449, 0.4595, 0.4733, 0.6441]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_torch(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecfbcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_backward(q: np.ndarray, k: np.ndarray, v: np.ndarray, dOut: np.ndarray):\n",
    "    B, N_HEADS, S, D_K = q.shape\n",
    "    scale = 1.0 / np.sqrt(D_K)\n",
    "\n",
    "    attn_scores = q.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) @ k.reshape(B,S,N_HEADS,D_K).swapaxes(1,2).swapaxes(2,3)\n",
    "    mask = np.tril(np.ones((B,N_HEADS,S,S)), k=0).astype(bool)\n",
    "    attn_scores_masked = np.where(mask, attn_scores, -np.inf)\n",
    "\n",
    "    scale = 1.0 / np.sqrt(q.shape[-1])\n",
    "    causal_attn = softmax(attn_scores_masked * scale)\n",
    "\n",
    "    dV = np.swapaxes(causal_attn, -2, -1) @ dOut.reshape(B,S,N_HEADS,D_K).swapaxes(1,2)\n",
    "    dAttn = dOut.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) @ np.swapaxes(v.reshape(B,S,N_HEADS,D_K).swapaxes(1,2), -2, -1)\n",
    "\n",
    "    dScores = softmax_backward_from_probs(causal_attn, dAttn)\n",
    "\n",
    "    dQ = dScores @ k.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) * scale\n",
    "    dK = np.swapaxes(dScores, -2, -1) @ q.reshape(B,S,N_HEADS,D_K).swapaxes(1,2) * scale\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3f7fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_multi_head_attention_backward(q_np, k_np, v_np, dOut_np):\n",
    "    \"\"\"PyTorch implementation of multi-head attention backward pass\"\"\"\n",
    "    q_torch = torch.from_numpy(q_np).float().requires_grad_(True)\n",
    "    k_torch = torch.from_numpy(k_np).float().requires_grad_(True)\n",
    "    v_torch = torch.from_numpy(v_np).float().requires_grad_(True)\n",
    "    dOut_torch = torch.from_numpy(dOut_np).float()\n",
    "\n",
    "    B, N_HEADS, S, D_K = q_torch.shape\n",
    "    print(q_torch.shape)\n",
    "\n",
    "    # Multi-head attention forward pass\n",
    "    mask = torch.triu(torch.ones(B, N_HEADS, S, S), diagonal=1)\n",
    "    attn_scores = q_torch @ k_torch.transpose(-2, -1) / np.sqrt(D_K)\n",
    "    causal_attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "    attn_weights = F.softmax(causal_attn_scores, dim=-1)\n",
    "    output_torch = attn_weights @ v_torch\n",
    "    print(output_torch.shape)\n",
    "\n",
    "    output_torch.backward(dOut_torch)\n",
    "\n",
    "    return (\n",
    "        q_torch.grad.detach().numpy(),\n",
    "        k_torch.grad.detach().numpy(),\n",
    "        v_torch.grad.detach().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8484acb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.01563788, -0.01123491,  0.0098367 , -0.00749334],\n",
       "         [ 0.00040571, -0.00184811,  0.01019378, -0.00243235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [-0.01165394,  0.04587054,  0.03697114,  0.00513161],\n",
       "         [-0.0142082 ,  0.03768114,  0.02941429, -0.00509489]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention_backward(q, k, v, dOut)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b254c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 4])\n",
      "torch.Size([2, 1, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.01563788, -0.01123491,  0.0098367 , -0.00749334],\n",
       "         [ 0.0004057 , -0.00184811,  0.01019377, -0.00243235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [-0.01165394,  0.04587054,  0.03697114,  0.00513161],\n",
       "         [-0.0142082 ,  0.03768113,  0.02941428, -0.0050949 ]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_multi_head_attention_backward(q, k, v, dOut)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4e499fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 24 into shape (1,3,1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmha_back_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdOut\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/TSFM/lecture-three/lecture-three/transformer.py:144\u001b[39m, in \u001b[36mmulti_head_attention_backward\u001b[39m\u001b[34m(q, k, v, dOut)\u001b[39m\n\u001b[32m    141\u001b[39m Dh = q.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    142\u001b[39m scale = \u001b[32m1.0\u001b[39m / np.sqrt(Dh)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m attn_scores = \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN_HEADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD_K\u001b[49m\u001b[43m)\u001b[49m.swapaxes(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m) @ k.reshape(B,S,N_HEADS,D_K).swapaxes(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m).swapaxes(\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m)\n\u001b[32m    145\u001b[39m mask = np.tril(np.ones(attn_scores.shape), k=\u001b[32m0\u001b[39m).astype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m    146\u001b[39m attn_scores_masked = np.where(mask, attn_scores, -np.inf)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 24 into shape (1,3,1,4)"
     ]
    }
   ],
   "source": [
    "mha_back_np(q, k, v, dOut)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66df4b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.01563788, -0.01123491,  0.0098367 , -0.00749334],\n",
       "        [ 0.0004057 , -0.00184811,  0.01019377, -0.00243235]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.01165394,  0.04587054,  0.03697114,  0.00513161],\n",
       "        [-0.0142082 ,  0.03768113,  0.02941428, -0.0050949 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_back_torch(q,k,v,dOut)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b63188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344104ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a129af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "711ca171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "714aa6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.swapaxes(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4666d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01127838, -0.00791101,  0.00586967, -0.00512856],\n",
       "        [ 0.01035594, -0.00737556,  0.00610205, -0.00486952],\n",
       "        [ 0.0004057 , -0.00184811,  0.01019377, -0.00243235]],\n",
       "\n",
       "       [[-0.02505569,  0.03117587,  0.02159204, -0.03093255],\n",
       "        [-0.01166164,  0.03306082,  0.0259736 , -0.00285432],\n",
       "        [-0.0142082 ,  0.03768113,  0.02941428, -0.0050949 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_back_torch(q, k, v, dOut)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "494bca70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 7, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,7,3],[4,5,6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "62c8d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "15ed3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "2f21e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  7,  10],\n",
       "        [ 15,  22]],\n",
       "\n",
       "       [[ 67,  78],\n",
       "        [ 91, 106]]])"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "2a1d279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,  78],\n",
       "       [ 91, 106]])"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] @ a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0bc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture-3-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
